# ==============================================
# AIãƒã‚¹ã‚­ãƒ³ã‚°ã‚¢ãƒ—ãƒªï¼ˆVision API + GPT-4o-miniï¼‰
# PDFå¯¾å¿œç‰ˆãƒ»UIä»˜ã
# ==============================================

import streamlit as st
import os
import io
import re
import json
import tempfile
import zipfile
from PIL import Image, ImageDraw
from openai import OpenAI  # â†â˜…ã“ã‚Œã‚’å¿…ãšè¿½åŠ ï¼

import os
import io
import json
import base64
import requests
from PIL import Image, ImageDraw

# -----------------------------
# Google Cloud Vision API (RESTç‰ˆ)
# -----------------------------
def get_vision_words(image_bytes):
    api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        st.error("âŒ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secrets ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        return [], ""

    try:
        image_base64 = base64.b64encode(image_bytes).decode("utf-8")
        endpoint = f"https://vision.googleapis.com/v1/images:annotate?key={api_key}"

        request_body = {
            "requests": [
                {
                    "image": {"content": image_base64},
                    "features": [{"type": "DOCUMENT_TEXT_DETECTION"}],
                }
            ]
        }

        response = requests.post(endpoint, json=request_body)
        result = response.json()

        if "error" in result:
            st.error(f"âŒ Vision API Error: {result['error'].get('message')}")
            return [], ""

        words = []
        text_annotation = result["responses"][0].get("fullTextAnnotation", {})
        full_text = text_annotation.get("text", "")

        for page in text_annotation.get("pages", []):
            for block in page.get("blocks", []):
                for para in block.get("paragraphs", []):
                    for word in para.get("words", []):
                        text = "".join([s["text"] for s in word.get("symbols", [])]).strip()
                        if not text:
                            continue
                        v = word.get("boundingBox", {}).get("vertices", [])
                        if len(v) >= 4:
                            x1, y1 = v[0].get("x", 0), v[0].get("y", 0)
                            x2, y2 = v[2].get("x", 0), v[2].get("y", 0)
                            words.append({"text": text, "bbox": (x1, y1, x2, y2)})

        return words, full_text

    except Exception as e:
        st.error(f"âŒ Vision API å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return [], ""

# ==============================================
# UIè¨­å®š
# ==============================================
st.set_page_config(page_title="AIãƒã‚¹ã‚¯")
st.title("AIãƒã‚¹ã‚­ãƒ³ã‚°ã‚¢ãƒ—ãƒª")
st.caption("Created by Kumagifï¼†Co.")

mask_style = st.radio("ãƒã‚¹ã‚¯æ–¹æ³•ã‚’é¸æŠ", ["é»’å¡—ã‚Š", "ãƒ¢ã‚¶ã‚¤ã‚¯"], horizontal=True)

uploaded_files = st.file_uploader(
    "ç”»åƒã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰",
    type=["jpg", "jpeg", "png", "pdf"],
    accept_multiple_files=True
)

st.write("ã¾ãšã¯ç”»åƒã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

st.markdown("""
---
### ğŸ’¡ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦
ã“ã®ã‚¢ãƒ—ãƒªã¯ **AIï¼‹OCRãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ** ã«ã‚ˆã‚Šã€æ¥µã‚ã¦é«˜ç²¾åº¦ã«å€‹äººæƒ…å ±ã‚’æ¤œå‡ºã—ãƒã‚¹ã‚­ãƒ³ã‚°ã—ã¾ã™ã€‚
""")

# ==============================================
# API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
# ==============================================
try:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    st.error(f"âŒ OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
def get_vision_client():
    try:
        return vision.ImageAnnotatorClient()
    except Exception as e:
        st.error(f"âŒ Vision API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise e

# ==============================================
# OCRï¼ˆVision APIï¼‰
# ==============================================
def get_vision_words(image_bytes):
    try:
        client_v = get_vision_client()
        image = vision.Image(content=image_bytes)
        response = client_v.document_text_detection(image=image)
        if response.error.message:
            raise Exception(response.error.message)
        words = []
        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for para in block.paragraphs:
                    for word in para.words:
                        text = "".join([s.text for s in word.symbols]).strip()
                        if not text:
                            continue
                        v = word.bounding_box.vertices
                        x1, y1 = v[0].x, v[0].y
                        x2, y2 = v[2].x, v[2].y
                        words.append({"text": text, "bbox": (x1, y1, x2, y2)})
        return words, response.full_text_annotation.text
    except Exception as e:
        st.error(f"âŒ Vision API ã‚¨ãƒ©ãƒ¼: {e}")
        return [], ""

# ==============================================
# GPTã§å€‹äººæƒ…å ±ã‚’æŠ½å‡º
# ==============================================
def ask_ai_for_sensitive_texts(ocr_text):
    prompt = f"""
ä»¥ä¸‹ã¯OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚
ãƒã‚¤ãƒŠãƒ³ãƒãƒ¼ã€ä¿é™ºè¨¼ç•ªå·ã€ä¿é™ºè€…ç•ªå·ã€åŸºç¤å¹´é‡‘ç•ªå·ã€ä½æ°‘ç¥¨ã‚³ãƒ¼ãƒ‰ãªã©
ã€Œå€‹äººæƒ…å ±ã«è©²å½“ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ç®‡æ‰€ã€ã‚’JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼š
[
  {{"text": "ãƒã‚¤ãƒŠãƒ³ãƒãƒ¼: 1234-5678-9012", "reason": "ãƒã‚¤ãƒŠãƒ³ãƒãƒ¼"}}
]

OCRçµæœ:
{ocr_text}
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        content = response.choices[0].message.content
        json_match = re.search(r"\[.*\]", content, re.S)
        if json_match:
            return json.loads(json_match.group(0))
        else:
            st.warning("âš ï¸ AIå‡ºåŠ›ãŒJSONå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return []
    except Exception as e:
        st.error(f"âŒ OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ==============================================
# é»’å¡—ã‚Šå‡¦ç†
# ==============================================
def apply_mask(image, words, sensitive_texts, mask_style="é»’å¡—ã‚Š"):
    im = image.convert("RGB")
    draw = ImageDraw.Draw(im)

    margin_x = 25  # â† å¹…ã‚’å°‘ã—å¤ªã‚ã«
    margin_y = 14

    combined_blocks = []
    temp_block = {"text": "", "bbox": None}

    def merge_bbox(b1, b2):
        if not b1:
            return b2
        x1 = min(b1[0], b2[0])
        y1 = min(b1[1], b2[1])
        x2 = max(b1[2], b2[2])
        y2 = max(b1[3], b2[3])
        return (x1, y1, x2, y2)

    for w in words:
        if re.match(r"^[0-9ï¼-ï¼™\-ãƒ¼]+$", w["text"]):
            temp_block["text"] += w["text"]
            temp_block["bbox"] = merge_bbox(temp_block["bbox"], w["bbox"])
        else:
            if temp_block["text"]:
                combined_blocks.append(temp_block)
            temp_block = {"text": "", "bbox": None}
    if temp_block["text"]:
        combined_blocks.append(temp_block)

    for s in sensitive_texts:
        s_clean = re.sub(r"[^0-9]", "", s["text"])
        if not s_clean:
            continue
        for block in combined_blocks:
            b_clean = re.sub(r"[^0-9]", "", block["text"])
            if s_clean in b_clean or s_clean[-8:] in b_clean or s_clean[-4:] in b_clean:
                x1, y1, x2, y2 = block["bbox"]
                x1 = max(0, x1 - margin_x)
                y1 = max(0, y1 - margin_y)
                x2 = min(im.width, x2 + margin_x)
                y2 = min(im.height, y2 + margin_y)
                draw.rectangle([x1, y1, x2, y2], fill="black")
    return im

# ==============================================
# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆPDFå¯¾å¿œï¼‰
# ==============================================
def convert_pdf_to_images(file_bytes):
    try:
        images = convert_from_bytes(file_bytes)
        img_bytes_list = []
        for img in images:
            buf = io.BytesIO()
            img.save(buf, format="PNG")
            img_bytes_list.append(buf.getvalue())
        return img_bytes_list
    except Exception as e:
        st.error(f"âŒ PDFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def process_image_file(file_bytes, mask_style="é»’å¡—ã‚Š"):
    words, ocr_text = get_vision_words(file_bytes)
    sensitive_texts = ask_ai_for_sensitive_texts(ocr_text)
    img = Image.open(io.BytesIO(file_bytes))
    masked = apply_mask(img, words, sensitive_texts, mask_style)
    buf = io.BytesIO()
    masked.save(buf, format="PNG")
    return buf.getvalue()

# ==============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†
# ==============================================
if uploaded_files:
    if st.button("ğŸ–¤ Vision + AIã§ãƒã‚¹ã‚­ãƒ³ã‚°å®Ÿè¡Œ"):
        with tempfile.TemporaryDirectory() as tmpdir:
            zip_path = os.path.join(tmpdir, "masked_outputs.zip")

            with zipfile.ZipFile(zip_path, "w") as zf:
                for f in uploaded_files:
                    st.write(f"å‡¦ç†ä¸­: {f.name}")
                    data = f.read()

                    if f.name.lower().endswith(".pdf"):
                        pdf_images = convert_pdf_to_images(data)
                        if not pdf_images:
                            st.warning(f"{f.name} ã®PDFå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            continue
                        for i, img_bytes in enumerate(pdf_images):
                            out_bytes = process_image_file(img_bytes, mask_style)
                            zf.writestr(f"masked_{os.path.splitext(f.name)[0]}_page{i+1}.png", out_bytes)
                    else:
                        out_bytes = process_image_file(data, mask_style)
                        zf.writestr(f"masked_{f.name}", out_bytes)

            with open(zip_path, "rb") as fp:
                st.download_button(
                    "ğŸ“¦ åŠ å·¥æ¸ˆã¿ZIPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=fp.read(),
                    file_name="masked_outputs_ai.zip"
                )
        st.success("âœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã—ãŸï¼")
